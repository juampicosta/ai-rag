# AI RAG Application

Bienvenido a la documentaciÃ³n de **AI RAG**, una aplicaciÃ³n de GeneraciÃ³n Aumentada por RecuperaciÃ³n (Retrival-Augmented Generation) diseÃ±ada para responder preguntas basÃ¡ndose en documentos procesados.

## ğŸš€ Â¿QuÃ© es RAG?

**RAG (Retrieval-Augmented Generation)** es una tÃ©cnica que mejora la precisiÃ³n y fiabilidad de los modelos de lenguaje (LLMs) proporcionÃ¡ndoles datos externos relevantes. En lugar de confiar Ãºnicamente en el conocimiento pre-entrenado del modelo, el sistema busca informaciÃ³n especÃ­fica en una base de conocimientos antes de generar una respuesta.

### El Flujo de RAG en este Proyecto

El flujo se divide en dos etapas principales: **IngestiÃ³n (PreparaciÃ³n)** y **GeneraciÃ³n (Consulta)**.

#### 1. IngestiÃ³n de Datos (`lib/ai/embedding`)

Esta etapa ocurre "offline" o antes de que el usuario haga preguntas. Su objetivo es convertir documentos en un formato que la IA pueda entender y buscar eficientemente.

1.  **Carga (Load):** Se leen archivos (actualmente PDFs) desde el sistema de archivos.
2.  **DivisiÃ³n (Chunking):** El texto completo se divide en fragmentos mÃ¡s pequeÃ±os y manejables ("chunks") utilizando `RecursiveCharacterTextSplitter` de LangChain.
    - _TamaÃ±o de chunk:_ 1000 caracteres.
    - _SuperposiciÃ³n (Overlap):_ 200 caracteres (para mantener el contexto entre cortes).
3.  **IncrustaciÃ³n (Embedding):** Cada chunk se convierte en un vector numÃ©rico (una lista de nÃºmeros) utilizando un modelo de embedding (`nomic-embed-text` vÃ­a Ollama). Estos vectores representan el significado semÃ¡ntico del texto.
4.  **Almacenamiento (Indexing):** Los chunks de texto y sus vectores correspondientes se guardan en **MongoDB Atlas**.

#### 2. GeneraciÃ³n de Respuesta (`lib/ai/retrieval.ts` y `generation.ts`)

Esta etapa ocurre cuando el usuario hace una pregunta.

1.  **Consulta (Query):** El usuario envÃ­a una pregunta.
2.  **VectorizaciÃ³n:** La pregunta se convierte en un vector usando el mismo modelo de embedding (`nomic-embed-text`).
3.  **RecuperaciÃ³n (Retrieval):** Se realiza una **BÃºsqueda Vectorial (Vector Search)** en MongoDB Atlas para encontrar los fragmentos de texto mÃ¡s similares semÃ¡nticamente a la pregunta.
4.  **Prompting:** Se construye un "System Prompt" que incluye:
    - Instrucciones para la IA (actuar como asistente, hablar en espaÃ±ol, etc.).
    - El **Contexto Recuperado** (los fragmentos de texto encontrados).
5.  **GeneraciÃ³n:** El LLM (`llama3.2` vÃ­a Ollama) recibe el prompt y genera una respuesta basada en la informaciÃ³n proporcionada.

---

## ğŸ› ï¸ Stack TecnolÃ³gico

- **Runtime:** Node.js & TypeScript
- **Base de Datos:** MongoDB Atlas (con Vector Search)
- **ODM:** Mongoose
- **AI SDK:** Vercel AI SDK (`ai`)
- **Proveedor AI:** Ollama (Local)
  - _Modelo de Chat:_ `llama3.2`
  - _Modelo de Embedding:_ `nomic-embed-text`
- **Procesamiento de Texto:** LangChain (`@langchain/textsplitters`), `pdf-parse`

---

## âš™ï¸ ConfiguraciÃ³n e InstalaciÃ³n

### Prerrequisitos

1.  **Node.js** (v18 o superior)
2.  **Ollama** instalado y corriendo localmente.
    - Descarga los modelos necesarios:
      ```bash
      ollama pull llama3.2
      ollama pull nomic-embed-text
      ```
3.  **MongoDB Atlas Cluster**: Debes tener una cuenta en MongoDB Atlas y un cluster configurado. AdemÃ¡s de setear la variable de entorno `DB_URL` en el archivo `.env`.

### ConfiguraciÃ³n de Base de Datos (MongoDB Atlas)

Para que la bÃºsqueda vectorial funcione, debes crear un **Indice de BÃºsqueda Vectorial** en tu colecciÃ³n `documents`.

1.  Ve a tu colecciÃ³n en Atlas.
2.  PestaÃ±a "Atlas Search" -> "Create Search Index".
3.  Selecciona "JSON Editor" y usa la siguiente configuraciÃ³n:

```json
{
  "fields": [
    {
      "numDimensions": 768,
      "path": "embedding",
      "similarity": "cosine",
      "type": "vector"
    }
  ]
}
```

### Variables de Entorno

Crea un archivo `.env` en la raÃ­z del proyecto:

```env
DB_URL="mongodb+srv://<usuario>:<password>@<cluster>.mongodb.net/<nombre-db>?retryWrites=true&w=majority"
NODE_ENV="development"
PORT=3000
```

### InstalaciÃ³n y EjecuciÃ³n

1.  **Instalar dependencias:**

    ```bash
    npm install
    # o
    pnpm install
    ```

2.  **Ingestar Documentos:**
    Coloca tus archivos PDF en la carpeta apropiada y ejecuta:

    ```bash
    npm run ingest
    ```

3.  **Iniciar Servidor de Desarrollo:**
    ```bash
    npm run dev
    ```

---

## ğŸ“‚ Estructura del Proyecto

```
/
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ ai/
â”‚       â”œâ”€â”€ embedding/
â”‚       â”‚   â”œâ”€â”€ ingest.ts       # LÃ³gica de procesamiento y vectorizaciÃ³n
â”‚       â”‚   â””â”€â”€ run-ingest.ts   # Script de ejecuciÃ³n de ingestiÃ³n
â”‚       â”œâ”€â”€ generation.ts       # LÃ³gica de generaciÃ³n de respuesta con LLM
â”‚       â””â”€â”€ retrieval.ts        # LÃ³gica de bÃºsqueda vectorial en MongoDB
â”œâ”€â”€ models/
â”‚   â””â”€â”€ Document.ts             # Esquema de Mongoose para documentos
â”œâ”€â”€ index.ts                    # Punto de entrada de la aplicaciÃ³n
â””â”€â”€ package.json
```

## ğŸ“ Notas Adicionales

- **Manejo de Errores:** Si el contexto no contiene informaciÃ³n suficiente, el modelo estÃ¡ instruido para indicarlo en lugar de alucinar una respuesta.
- **Temperatura:** Se usa una temperatura baja (0.1) para maximizar la consistencia factual.
